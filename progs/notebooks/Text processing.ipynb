{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48d774be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today\n",
      "is\n",
      "the\n",
      "anniversary\n",
      "of\n",
      "the\n",
      "publication\n",
      "of\n",
      "Robert\n",
      "Frost\n",
      "’s\n",
      "iconic\n",
      "poem\n",
      "“\n",
      "Stopping\n",
      "by\n",
      "Woods\n",
      "on\n",
      "a\n",
      "Snowy\n",
      "Evening\n",
      ",\n",
      "”\n",
      "a\n",
      "fact\n",
      "that\n",
      "spurred\n",
      "the\n",
      "Literary\n",
      "Hub\n",
      "office\n",
      "into\n",
      "a\n",
      "long\n",
      "conversation\n",
      "about\n",
      "their\n",
      "favorite\n",
      "poems\n",
      ",\n",
      "the\n",
      "most\n",
      "iconic\n",
      "poems\n",
      "written\n",
      "in\n",
      "English\n",
      ",\n",
      "and\n",
      "which\n",
      "poems\n",
      "we\n",
      "should\n",
      "all\n",
      "have\n",
      "already\n",
      "read\n",
      "(\n",
      "or\n",
      "at\n",
      "least\n",
      "be\n",
      "reading\n",
      "next\n",
      ")\n",
      ".\n",
      "Turns\n",
      "out\n",
      ",\n",
      "despite\n",
      "frequent\n",
      "(\n",
      "false\n",
      ")\n",
      "claims\n",
      "that\n",
      "poetry\n",
      "is\n",
      "dead\n",
      "and/or\n",
      "irrelevant\n",
      "and/or\n",
      "boring\n",
      ",\n",
      "there\n",
      "are\n",
      "plenty\n",
      "of\n",
      "poems\n",
      "that\n",
      "have\n",
      "sunk\n",
      "deep\n",
      "into\n",
      "our\n",
      "collective\n",
      "consciousness\n",
      "as\n",
      "cultural\n",
      "icons\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp=spacy.blank(\"en\")\n",
    "\n",
    "doc=nlp(\"Today is the anniversary of the publication of Robert Frost’s iconic poem “Stopping by Woods on a Snowy Evening,” a fact that spurred the Literary Hub office into a long conversation about their favorite poems, the most iconic poems written in English, and which poems we should all have already read (or at least be reading next). Turns out, despite frequent (false) claims that poetry is dead and/or irrelevant and/or boring, there are plenty of poems that have sunk deep into our collective consciousness as cultural icons.\")\n",
    "\n",
    "for sentence in doc:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdc8439b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n",
      "I\n",
      "am\n",
      "Roronoa\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp=spacy.blank(\"en\")\n",
    "\n",
    "doc=nlp(\"Hi I am Roronoa\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3febf6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today\n",
      "is\n",
      "the\n",
      "anniversary\n",
      "of\n",
      "the\n",
      "publication\n",
      "of\n",
      "Robert\n",
      "Frost\n",
      "’s\n",
      "iconic\n",
      "poem\n",
      "“\n",
      "Stopping\n",
      "by\n",
      "Woods\n",
      "on\n",
      "a\n",
      "Snowy\n",
      "Evening\n",
      ",\n",
      "”\n",
      "a\n",
      "fact\n",
      "that\n",
      "spurred\n",
      "the\n",
      "Literary\n",
      "Hub\n",
      "office\n",
      "into\n",
      "a\n",
      "long\n",
      "conversation\n",
      "about\n",
      "their\n",
      "favorite\n",
      "poems\n",
      ",\n",
      "the\n",
      "most\n",
      "iconic\n",
      "poems\n",
      "written\n",
      "in\n",
      "English\n",
      ",\n",
      "and\n",
      "which\n",
      "poems\n",
      "we\n",
      "should\n",
      "all\n",
      "have\n",
      "already\n",
      "read\n",
      "(\n",
      "or\n",
      "at\n",
      "least\n",
      "be\n",
      "reading\n",
      "next\n",
      ")\n",
      ".\n",
      "Turns\n",
      "out\n",
      ",\n",
      "despite\n",
      "frequent\n",
      "(\n",
      "false\n",
      ")\n",
      "claims\n",
      "that\n",
      "poetry\n",
      "is\n",
      "dead\n",
      "and/or\n",
      "irrelevant\n",
      "and/or\n",
      "boring\n",
      ",\n",
      "there\n",
      "are\n",
      "plenty\n",
      "of\n",
      "poems\n",
      "that\n",
      "have\n",
      "sunk\n",
      "deep\n",
      "into\n",
      "our\n",
      "collective\n",
      "consciousness\n",
      "as\n",
      "cultural\n",
      "icons\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp=spacy.blank(\"en\")\n",
    "\n",
    "doc=nlp(\"Today is the anniversary of the publication of Robert Frost’s iconic poem “Stopping by Woods on a Snowy Evening,” a fact that spurred the Literary Hub office into a long conversation about their favorite poems, the most iconic poems written in English, and which poems we should all have already read (or at least be reading next). Turns out, despite frequent (false) claims that poetry is dead and/or irrelevant and/or boring, there are plenty of poems that have sunk deep into our collective consciousness as cultural icons. \")\n",
    "\n",
    "for token in doc:\n",
    " print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01f7de66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and', 'was', 'ourselves', 'cannot', 'move', 'again', 'whenever', 'keep', 'also', 'sometime', 'many', 'make', 'you', 'with', 'eight', 'become', 'all', 'his', 'over', 'perhaps', '‘s', 'below', 'serious', 'the', \"'ve\", 'show', '’s', 'hundred', 'herself', 'whereafter', 'whom', 'therein', 'take', 'n‘t', 'one', '’ve', \"'d\", 'for', 'just', 'although', 'being', 'mine', 'own', \"'s\", 'more', 'above', 'hence', 'among', 'hereafter', 'yet', 'bottom', 'that', '’m', 'yours', 'eleven', 'otherwise', 'yourself', '‘ve', 'herein', '‘m', 'her', 'whence', 'meanwhile', 'top', 'first', 'doing', 'through', 'she', 'have', 'everyone', 'to', 'made', 'whither', 'than', 'nor', 'themselves', 'were', 'as', 're', 'but', 'without', 'becomes', '‘ll', 'your', 'yourselves', 'why', 'both', 'whatever', 'every', 'further', 'side', 'seems', 'anyway', 'myself', 'i', 'always', 'never', 'various', 'often', 'thru', 'thereupon', 'towards', 'toward', 'anyhow', 'between', 'be', 'up', '’re', 'on', 'enough', 'third', 'an', 'somewhere', 'full', \"'ll\", 'whereby', 'give', 'seeming', 'thus', 'four', 'they', 'there', 'ours', 'it', 'nowhere', 'namely', 'either', 'who', 'twelve', 'while', 'might', 'else', 'whereas', 'something', 'had', 'back', 'two', 'must', 'get', 'such', 'behind', 'latterly', 'will', 'afterwards', 'call', 'very', 'somehow', 'if', 'no', 'their', 'mostly', 'most', 'nevertheless', 'say', 'least', 'please', 'along', 'across', 'whereupon', 'besides', 'from', 'due', 'almost', 'before', 'itself', 'several', 'anyone', 'could', 'really', 'each', 'wherein', 'indeed', 'less', 'unless', 'beforehand', 'few', 'too', \"'re\", 'front', 'hers', 'a', 'ca', 'rather', 'forty', 'after', 'elsewhere', 'which', 'last', 'wherever', 'off', '’d', 'put', 'about', 'beside', 'is', 'none', 'or', 'part', 'hereupon', 'someone', 'those', 'three', 'noone', 'go', 'by', 'regarding', 'even', 'done', 'me', 'into', 'alone', 'how', 'thereby', 'fifteen', 'now', 'my', 'since', 'ever', 'thereafter', 'when', 'empty', 'until', 'became', 'whose', '‘d', 'himself', 'quite', 'onto', 'may', 'within', 'however', 'our', 'do', 'has', 'n’t', 'in', 'these', 'does', 'formerly', 'name', 'another', 'upon', 'did', '‘re', 'already', 'together', 'amount', 'hereby', \"n't\", 'should', 'because', 'during', 'everywhere', 'using', 'would', 'any', 'via', 'former', 'sometimes', 'what', 'used', 'not', 'well', 'everything', 'them', 'others', 'we', 'per', 'us', 'only', \"'m\", 'at', 'sixty', 'whole', 'where', 'whether', 'its', 'here', 'are', 'been', 'becoming', 'five', 'down', 'moreover', 'next', 'seem', 'seemed', 'except', 'therefore', 'around', 'amongst', 'of', 'though', 'nothing', 'latter', 'neither', 'beyond', 'can', 'anything', 'much', 'fifty', 'throughout', 'under', 'against', 'he', 'whoever', 'see', 'six', 'once', 'him', 'twenty', 'ten', 'this', 'nobody', 'some', 'same', 'then', 'other', 'nine', 'am', 'anywhere', 'out', 'so', 'thence', 'still', '’ll'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "    \n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70907ecd",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E053] Could not read meta.json from .",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m doc\u001b[38;5;241m=\u001b[39mnlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToday is the anniversary of the publication of Robert Frost’s iconic poem “Stopping by Woods on a Snowy Evening,” a fact that spurred the Literary Hub office into a long conversation about their favorite poems, the most iconic poems written in English, and which poems we should all have already read (or at least be reading next). Turns out, despite frequent (false) claims that poetry is dead and/or irrelevant and/or boring, there are plenty of poems that have sunk deep into our collective consciousness as cultural icons.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\spacy\\__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     31\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\spacy\\util.py:444\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_package(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m Path(name)\u001b[38;5;241m.\u001b[39mexists():  \u001b[38;5;66;03m# path to model data directory\u001b[39;00m\n\u001b[1;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexists\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# Path or Path-like to model data\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\spacy\\util.py:512\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE052\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mmodel_path))\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m meta:\n\u001b[1;32m--> 512\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    513\u001b[0m config_path \u001b[38;5;241m=\u001b[39m model_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.cfg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    514\u001b[0m overrides \u001b[38;5;241m=\u001b[39m dict_to_dot(config)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\spacy\\util.py:906\u001b[0m, in \u001b[0;36mget_model_meta\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get model meta.json from a directory path and validate its contents.\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \n\u001b[0;32m    902\u001b[0m \u001b[38;5;124;03mpath (str / Path): Path to model directory.\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;124;03mRETURNS (Dict[str, Any]): The model's meta data.\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    905\u001b[0m model_path \u001b[38;5;241m=\u001b[39m ensure_path(path)\n\u001b[1;32m--> 906\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\spacy\\util.py:866\u001b[0m, in \u001b[0;36mload_meta\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE052\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mpath\u001b[38;5;241m.\u001b[39mparent))\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m--> 866\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE053\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mpath\u001b[38;5;241m.\u001b[39mparent, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta.json\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    867\u001b[0m meta \u001b[38;5;241m=\u001b[39m srsly\u001b[38;5;241m.\u001b[39mread_json(path)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m setting \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[1;31mOSError\u001b[0m: [E053] Could not read meta.json from ."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"\")\n",
    "\n",
    "doc=nlp(\"Today is the anniversary of the publication of Robert Frost’s iconic poem “Stopping by Woods on a Snowy Evening,” a fact that spurred the Literary Hub office into a long conversation about their favorite poems, the most iconic poems written in English, and which poems we should all have already read (or at least be reading next). Turns out, despite frequent (false) claims that poetry is dead and/or irrelevant and/or boring, there are plenty of poems that have sunk deep into our collective consciousness as cultural icons.\")\n",
    "\n",
    "for token in doc:\n",
    "    if nlp.vocab[token.text].is_stop:\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c557f7a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdoc\u001b[49m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mvocab[token\u001b[38;5;241m.\u001b[39mtext]\u001b[38;5;241m.\u001b[39mis_stop:\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(token\u001b[38;5;241m.\u001b[39mtext)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if not nlp.vocab[token.text].is_stop:\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34ff29f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n\u001b[0;32m      5\u001b[0m stemmer\u001b[38;5;241m=\u001b[39mPorterStemmer()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "words=[\"eating\",\"eats\",\"ate\",\"adjustable\",\"ability\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word,\"|\",stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70e2f271",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m nlp\u001b[38;5;241m=\u001b[39m\u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m doc\u001b[38;5;241m=\u001b[39mnlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meating eats ate ability\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\spacy\\__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     31\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\spacy\\util.py:449\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc=nlp(\"eating eats ate ability\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\"|\",token.lemma_,\"|\",token.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d47424da",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m nlp\u001b[38;5;241m=\u001b[39m\u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m doc\u001b[38;5;241m=\u001b[39mnlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTwo roads diverged in a wood, and I, I took the one less travelled by, and that has made all the difference.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\spacy\\__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     31\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\spacy\\util.py:449\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc=nlp(\"Two roads diverged in a wood, and I, I took the one less travelled by, and that has made all the difference.\")\n",
    "\n",
    "for token in doc:\n",
    "     print(token,\"|\",token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ade2600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fail', 'fail']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"If you set your goals ridiculously high and it's a failure, you will fail above everyone else's success. -James Cameron\"\n",
    "\n",
    "x = re.findall(\"fail\", text)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e90af59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first white-space character is located in position: <re.Match object; span=(8, 9), match=' '>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "txt = \"Lokmanya Tilak College of Engineering\"\n",
    "\n",
    "x = re.search(\"\\s\", txt)\n",
    "\n",
    "print(\"The first white-space character is located in position:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "679ec200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CLASS:SE', 'DIV:B', 'AY:2022-23']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "txt = \"CLASS:SE DIV:B AY:2022-23\"\n",
    "\n",
    "x = re.split(\"\\s\", txt)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3d9d87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'a', 'm', 'h', 'i', 'r', 'i', 'n']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hi I am Shirin\"\n",
    "\n",
    "x = re.findall(\"[a-z]\", text)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2608c8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '5', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"I want to buy book which costs 1500 Rs\"\n",
    "\n",
    "x = re.findall(\"\\d\", text)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c2512bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"hello students...\"\n",
    "\n",
    "x = re.findall(\"he..o\", text)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72b4525a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the string starts with 'Good morning'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"GM lets start the discussion\"\n",
    "\n",
    "x = re.findall(\"^GM\", text)\n",
    "\n",
    "if x:\n",
    "    print(\"Yes, the string starts with 'Good morning'\")\n",
    "\n",
    "else:\n",
    "    print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1482e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the string ends with 'Mumbai'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Lokmanya Tilak College of Engineering, established in 1994 is an engineering college in Navi Mumbai\"\n",
    "\n",
    "x = re.findall(\"Mumbai$\", text)\n",
    "\n",
    "if x:\n",
    "    print(\"Yes, the string ends with 'Mumbai'\")\n",
    "\n",
    "else:\n",
    "    print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55137421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"hello \"\n",
    "\n",
    "x = re.findall(\"he.*o\", text)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b124213d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"hello\"\n",
    "\n",
    "x = re.findall(\"he.+o\", text)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4a583d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jelly']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"jelly\"\n",
    "\n",
    "x = re.findall(\"j.{3}y\", text)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "582787ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 9\n",
      "Found match for quick at index 4\n",
      "Replaced lazy with energetic : The quick brown fox jumps over the energetic dog.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define a string of text\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Split the text into a list of words\n",
    "words = text.split()\n",
    "\n",
    "# Count the number of words in the text\n",
    "num_words = len(words)\n",
    "print(\"Number of words:\", num_words)\n",
    "\n",
    "# Search for a word in the text using regular expressions\n",
    "search_word = \"quick\"\n",
    "match = re.search(search_word, text)\n",
    "if match:\n",
    "  print(\"Found match for\", search_word, \"at index\", match.start())\n",
    "else:\n",
    "  print(\"No match found for\", search_word)\n",
    "\n",
    "# Replace a word in the text using regular expressions\n",
    "replace_word = \"lazy\"\n",
    "new_text = re.sub(replace_word, \"energetic\", text)\n",
    "print(\"Replaced\", replace_word, \"with\", \"energetic\", \":\", new_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
