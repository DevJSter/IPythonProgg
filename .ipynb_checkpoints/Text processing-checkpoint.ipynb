{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48d774be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today\n",
      "is\n",
      "the\n",
      "anniversary\n",
      "of\n",
      "the\n",
      "publication\n",
      "of\n",
      "Robert\n",
      "Frost\n",
      "’s\n",
      "iconic\n",
      "poem\n",
      "“\n",
      "Stopping\n",
      "by\n",
      "Woods\n",
      "on\n",
      "a\n",
      "Snowy\n",
      "Evening\n",
      ",\n",
      "”\n",
      "a\n",
      "fact\n",
      "that\n",
      "spurred\n",
      "the\n",
      "Literary\n",
      "Hub\n",
      "office\n",
      "into\n",
      "a\n",
      "long\n",
      "conversation\n",
      "about\n",
      "their\n",
      "favorite\n",
      "poems\n",
      ",\n",
      "the\n",
      "most\n",
      "iconic\n",
      "poems\n",
      "written\n",
      "in\n",
      "English\n",
      ",\n",
      "and\n",
      "which\n",
      "poems\n",
      "we\n",
      "should\n",
      "all\n",
      "have\n",
      "already\n",
      "read\n",
      "(\n",
      "or\n",
      "at\n",
      "least\n",
      "be\n",
      "reading\n",
      "next\n",
      ")\n",
      ".\n",
      "Turns\n",
      "out\n",
      ",\n",
      "despite\n",
      "frequent\n",
      "(\n",
      "false\n",
      ")\n",
      "claims\n",
      "that\n",
      "poetry\n",
      "is\n",
      "dead\n",
      "and/or\n",
      "irrelevant\n",
      "and/or\n",
      "boring\n",
      ",\n",
      "there\n",
      "are\n",
      "plenty\n",
      "of\n",
      "poems\n",
      "that\n",
      "have\n",
      "sunk\n",
      "deep\n",
      "into\n",
      "our\n",
      "collective\n",
      "consciousness\n",
      "as\n",
      "cultural\n",
      "icons\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp=spacy.blank(\"en\")\n",
    "\n",
    "doc=nlp(\"Today is the anniversary of the publication of Robert Frost’s iconic poem “Stopping by Woods on a Snowy Evening,” a fact that spurred the Literary Hub office into a long conversation about their favorite poems, the most iconic poems written in English, and which poems we should all have already read (or at least be reading next). Turns out, despite frequent (false) claims that poetry is dead and/or irrelevant and/or boring, there are plenty of poems that have sunk deep into our collective consciousness as cultural icons.\")\n",
    "\n",
    "for sentence in doc:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdc8439b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n",
      "I\n",
      "am\n",
      "Roronoa\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp=spacy.blank(\"en\")\n",
    "\n",
    "doc=nlp(\"Hi I am Roronoa\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3febf6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today\n",
      "is\n",
      "the\n",
      "anniversary\n",
      "of\n",
      "the\n",
      "publication\n",
      "of\n",
      "Robert\n",
      "Frost\n",
      "’s\n",
      "iconic\n",
      "poem\n",
      "“\n",
      "Stopping\n",
      "by\n",
      "Woods\n",
      "on\n",
      "a\n",
      "Snowy\n",
      "Evening\n",
      ",\n",
      "”\n",
      "a\n",
      "fact\n",
      "that\n",
      "spurred\n",
      "the\n",
      "Literary\n",
      "Hub\n",
      "office\n",
      "into\n",
      "a\n",
      "long\n",
      "conversation\n",
      "about\n",
      "their\n",
      "favorite\n",
      "poems\n",
      ",\n",
      "the\n",
      "most\n",
      "iconic\n",
      "poems\n",
      "written\n",
      "in\n",
      "English\n",
      ",\n",
      "and\n",
      "which\n",
      "poems\n",
      "we\n",
      "should\n",
      "all\n",
      "have\n",
      "already\n",
      "read\n",
      "(\n",
      "or\n",
      "at\n",
      "least\n",
      "be\n",
      "reading\n",
      "next\n",
      ")\n",
      ".\n",
      "Turns\n",
      "out\n",
      ",\n",
      "despite\n",
      "frequent\n",
      "(\n",
      "false\n",
      ")\n",
      "claims\n",
      "that\n",
      "poetry\n",
      "is\n",
      "dead\n",
      "and/or\n",
      "irrelevant\n",
      "and/or\n",
      "boring\n",
      ",\n",
      "there\n",
      "are\n",
      "plenty\n",
      "of\n",
      "poems\n",
      "that\n",
      "have\n",
      "sunk\n",
      "deep\n",
      "into\n",
      "our\n",
      "collective\n",
      "consciousness\n",
      "as\n",
      "cultural\n",
      "icons\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp=spacy.blank(\"en\")\n",
    "\n",
    "doc=nlp(\"Today is the anniversary of the publication of Robert Frost’s iconic poem “Stopping by Woods on a Snowy Evening,” a fact that spurred the Literary Hub office into a long conversation about their favorite poems, the most iconic poems written in English, and which poems we should all have already read (or at least be reading next). Turns out, despite frequent (false) claims that poetry is dead and/or irrelevant and/or boring, there are plenty of poems that have sunk deep into our collective consciousness as cultural icons. \")\n",
    "\n",
    "for token in doc:\n",
    " print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01f7de66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'at', 'elsewhere', 'give', 'take', 'ca', 'eleven', 'again', 'behind', 'becomes', 'anyway', '‘re', 'whoever', 'go', 'herself', 'n‘t', 'too', '’ve', 'over', 'whereafter', 'say', 'per', 'a', 'other', 'forty', 'up', 'now', 'seeming', 'various', 'put', 'hereupon', 'if', 'yours', \"'s\", 'therein', 'former', 'can', 'name', 'last', 'towards', 'they', 'still', 'will', 'call', 'everywhere', 'rather', 'third', 'ourselves', 'ever', 'became', 'enough', 'an', 'may', 'nor', 'is', 'anyone', 'our', 'beyond', 'somewhere', 'their', 'do', \"'d\", 'doing', 'show', 'across', \"'m\", 'off', 'wherever', 'three', 'herein', 'those', 'toward', 'somehow', \"'ll\", 'hereafter', 'on', 'me', 'you', 'everyone', 'am', 'please', 'her', 'two', 'although', 'who', 'does', 'nevertheless', 'really', 'anything', 'each', 'along', 'afterwards', 'wherein', 'serious', 'further', 'otherwise', 'this', 'latterly', 'becoming', 'many', 'twenty', \"n't\", 'in', 'where', 'whom', 'cannot', 'hers', 'to', 'have', 'whither', 'hereby', 'since', \"'re\", 'else', 'into', 'within', 'such', 'ours', 'of', 'its', 'it', 'meanwhile', 'beforehand', 'top', 'between', 'formerly', 'hence', 'well', 'via', 'never', 'seems', 'all', 'against', 'alone', 'whereas', 'back', 'whenever', 'latter', 'few', 'i', 'very', 'upon', \"'ve\", 'not', 'anywhere', 'one', 'above', 'yet', 'always', 'by', 'also', 'beside', 'he', 'much', 'fifty', 'my', 'n’t', 'once', 'whole', 'were', 'mostly', 'perhaps', 'thence', 'until', 'itself', 'just', 'your', 'side', '’d', 'below', 'though', 'was', 'someone', 'keep', 'nine', 'seemed', 'only', 'none', 'that', 'them', 'for', 'being', 'fifteen', 'regarding', 'least', 'more', '‘d', 'either', 'except', 'while', 'ten', 'sometime', 'myself', 'however', 'get', 'under', 'first', 'own', 'themselves', 'throughout', 'whatever', 'see', 'often', 'even', 'whereby', 'when', 'the', 'could', 'front', 'or', 'his', 'whereupon', 'then', '’ll', 'indeed', 'noone', 'besides', 'amount', 'next', 'what', 'these', 'been', 'whence', 'anyhow', 'part', 'six', '‘s', 'because', 'thus', 'used', 'during', 'made', 'must', 'due', '’s', 'already', 'bottom', 'through', 'himself', 'nothing', 'something', 'without', 'moreover', 'yourself', 'thereafter', 'and', 'whether', 're', 'him', 'namely', 'had', 'us', 'twelve', 'others', 'onto', 'using', 'another', 'down', 'five', '’m', 'has', 'we', '‘m', 'might', 'same', 'there', 'around', '‘ll', 'empty', 'with', 'thereupon', 'from', 'less', 'amongst', 'but', 'how', 'every', 'than', 'some', 'any', 'why', 'make', 'out', 'several', 'be', 'which', 'together', 'unless', 'among', 'should', 'eight', '’re', 'full', 'almost', 'thru', 'four', 'are', 'as', 'before', 'neither', 'did', 'nowhere', 'everything', 'yourselves', 'most', 'move', 'thereby', 'here', 'sixty', 'sometimes', 'both', 'therefore', 'about', 'hundred', 'quite', 'mine', 'become', '‘ve', 'so', 'after', 'she', 'nobody', 'would', 'done', 'seem', 'no', 'whose'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "    \n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70907ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n",
      "the\n",
      "of\n",
      "the\n",
      "of\n",
      "’s\n",
      "by\n",
      "on\n",
      "a\n",
      "a\n",
      "that\n",
      "the\n",
      "into\n",
      "a\n",
      "about\n",
      "their\n",
      "the\n",
      "most\n",
      "in\n",
      "and\n",
      "which\n",
      "we\n",
      "should\n",
      "all\n",
      "have\n",
      "already\n",
      "or\n",
      "at\n",
      "least\n",
      "be\n",
      "next\n",
      "out\n",
      "that\n",
      "is\n",
      "there\n",
      "are\n",
      "of\n",
      "that\n",
      "have\n",
      "into\n",
      "our\n",
      "as\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc=nlp(\"Today is the anniversary of the publication of Robert Frost’s iconic poem “Stopping by Woods on a Snowy Evening,” a fact that spurred the Literary Hub office into a long conversation about their favorite poems, the most iconic poems written in English, and which poems we should all have already read (or at least be reading next). Turns out, despite frequent (false) claims that poetry is dead and/or irrelevant and/or boring, there are plenty of poems that have sunk deep into our collective consciousness as cultural icons.\")\n",
    "\n",
    "for token in doc:\n",
    "    if nlp.vocab[token.text].is_stop:\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c557f7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today\n",
      "anniversary\n",
      "publication\n",
      "Robert\n",
      "Frost\n",
      "iconic\n",
      "poem\n",
      "“\n",
      "Stopping\n",
      "Woods\n",
      "Snowy\n",
      "Evening\n",
      ",\n",
      "”\n",
      "fact\n",
      "spurred\n",
      "Literary\n",
      "Hub\n",
      "office\n",
      "long\n",
      "conversation\n",
      "favorite\n",
      "poems\n",
      ",\n",
      "iconic\n",
      "poems\n",
      "written\n",
      "English\n",
      ",\n",
      "poems\n",
      "read\n",
      "(\n",
      "reading\n",
      ")\n",
      ".\n",
      "Turns\n",
      ",\n",
      "despite\n",
      "frequent\n",
      "(\n",
      "false\n",
      ")\n",
      "claims\n",
      "poetry\n",
      "dead\n",
      "and/or\n",
      "irrelevant\n",
      "and/or\n",
      "boring\n",
      ",\n",
      "plenty\n",
      "poems\n",
      "sunk\n",
      "deep\n",
      "collective\n",
      "consciousness\n",
      "cultural\n",
      "icons\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if not nlp.vocab[token.text].is_stop:\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34ff29f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "eats | eat\n",
      "ate | ate\n",
      "adjustable | adjust\n",
      "ability | abil\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "words=[\"eating\",\"eats\",\"ate\",\"adjustable\",\"ability\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word,\"|\",stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70e2f271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eating | 12092082220177030354\n",
      "eats | eat | 9837207709914848172\n",
      "ate | eat | 9837207709914848172\n",
      "ability | ability | 11565809527369121409\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc=nlp(\"eating eats ate ability\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\"|\",token.lemma_,\"|\",token.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d47424da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two | two\n",
      "roads | road\n",
      "diverged | diverge\n",
      "in | in\n",
      "a | a\n",
      "wood | wood\n",
      ", | ,\n",
      "and | and\n",
      "I | I\n",
      ", | ,\n",
      "I | I\n",
      "took | take\n",
      "the | the\n",
      "one | one\n",
      "less | less\n",
      "travelled | travel\n",
      "by | by\n",
      ", | ,\n",
      "and | and\n",
      "that | that\n",
      "has | have\n",
      "made | make\n",
      "all | all\n",
      "the | the\n",
      "difference | difference\n",
      ". | .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc=nlp(\"Two roads diverged in a wood, and I, I took the one less travelled by, and that has made all the difference.\")\n",
    "\n",
    "for token in doc:\n",
    "     print(token,\"|\",token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ade2600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fail', 'fail']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"If you set your goals ridiculously high and it's a failure, you will fail above everyone else's success. -James Cameron\"\n",
    "\n",
    "x = re.findall(\"fail\", text)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e90af59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first white-space character is located in position: <re.Match object; span=(8, 9), match=' '>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "txt = \"Lokmanya Tilak College of Engineering\"\n",
    "\n",
    "x = re.search(\"\\s\", txt)\n",
    "\n",
    "print(\"The first white-space character is located in position:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "679ec200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CLASS:SE', 'DIV:B', 'AY:2022-23']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "txt = \"CLASS:SE DIV:B AY:2022-23\"\n",
    "\n",
    "x = re.split(\"\\s\", txt)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3d9d87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'a', 'm', 'h', 'i', 'r', 'i', 'n']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hi I am Shirin\"\n",
    "\n",
    "x = re.findall(\"[a-z]\", text)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2608c8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '5', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"I want to buy book which costs 1500 Rs\"\n",
    "\n",
    "x = re.findall(\"\\d\", text)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c2512bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"hello students...\"\n",
    "\n",
    "x = re.findall(\"he..o\", text)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72b4525a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the string starts with 'Good morning'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"GM lets start the discussion\"\n",
    "\n",
    "x = re.findall(\"^GM\", text)\n",
    "\n",
    "if x:\n",
    "    print(\"Yes, the string starts with 'Good morning'\")\n",
    "\n",
    "else:\n",
    "    print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1482e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the string ends with 'Mumbai'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Lokmanya Tilak College of Engineering, established in 1994 is an engineering college in Navi Mumbai\"\n",
    "\n",
    "x = re.findall(\"Mumbai$\", text)\n",
    "\n",
    "if x:\n",
    "    print(\"Yes, the string ends with 'Mumbai'\")\n",
    "\n",
    "else:\n",
    "    print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55137421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"hello \"\n",
    "\n",
    "x = re.findall(\"he.*o\", text)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b124213d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"hello\"\n",
    "\n",
    "x = re.findall(\"he.+o\", text)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4a583d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jelly']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"jelly\"\n",
    "\n",
    "x = re.findall(\"j.{3}y\", text)\n",
    "\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
